{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd1f496a-f307-4caa-a597-60a6630589e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Arquitetura Medalhão — Camada Bronze\n",
    "## Notebook: landing_to_bronze\n",
    "\n",
    "Este notebook realiza a **ingestão dos dados brutos (Landing Layer)** para a **Camada Bronze** do catálogo `medalhao`.\n",
    "\n",
    "A camada Bronze é responsável por armazenar os dados **exatamente como vieram da fonte**, apenas adicionando metadados técnicos, como o `ingestion_timestamp`.  \n",
    "Todas as tabelas são armazenadas em formato **Delta Lake**, dentro do schema `bronze`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db260c98-769a-4cd4-9e2d-a2db53553c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuração inicial\n",
    "\n",
    "Nesta etapa, são configurados o catálogo e o schema (`medalhao.bronze`) onde as tabelas serão criadas.  \n",
    "Se o database ainda não existir, ele será criado automaticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fc49a7-6e7f-4cd1-80d7-66f3f28d2802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--DROP CATALOG IF EXISTS medalhao CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719fc874-c91b-4136-a7bf-3c4dfb32e201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--CREATE CATALOG IF NOT EXISTS medalhao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d0eea1-4dfd-4340-876f-5e6e3b86c2d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database medalhao.bronze criado/verificado com sucesso.\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "catalogo = \"medalhao\"\n",
    "bronze_db_name = \"bronze\"\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalogo}.{bronze_db_name}\")\n",
    "print(f\"Database {catalogo}.{bronze_db_name} criado/verificado com sucesso.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08295745-ff58-49b6-a0c0-a7903bcc9667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Função de ingestão de CSVs\n",
    "\n",
    "A função `ingest_csv()` é responsável por:\n",
    "- Ler os arquivos CSV da camada Landing;\n",
    "- Validar se o arquivo não está vazio;\n",
    "- Adicionar a coluna `ingestion_timestamp` com o horário da carga;\n",
    "- Gravar os dados no formato Delta na tabela correspondente da camada Bronze.\n",
    "\n",
    "Essa função será utilizada para os 9 arquivos do dataset **Olist**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb4c7945-8ee7-43fa-b1e9-c06630d3aa9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_csv(nome_arquivo, nome_tabela):\n",
    "    try:\n",
    "        landing_path = f\"/Volumes/{catalogo}/default/landing/{nome_arquivo}\"\n",
    "        table_name = nome_tabela\n",
    "        \n",
    "        df = spark.read.csv(landing_path, header=True, inferSchema=True)\n",
    "\n",
    "        #se arquivo estiver vazio, levanta uma exceção\n",
    "        if df.count() == 0:\n",
    "            raise ValueError(f\"O arquivo {nome_arquivo} está vazio ou não pôde ser lido.\")\n",
    "\n",
    "        # adiciona timestamp\n",
    "        df_with_metadata = df.withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "\n",
    "        #formato delta\n",
    "        df_with_metadata.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(f\"{catalogo}.{bronze_db_name}.{table_name}\")\n",
    "\n",
    "        print(f\"Tabela {bronze_db_name}.{nome_tabela} criada com sucesso\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {nome_tabela}: {str(e)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2246b7e7-b3a3-4b22-a849-40af9c73b123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingestão dos arquivos Landing → Bronze\n",
    "\n",
    "Aqui são definidos os nomes dos arquivos CSV e suas tabelas correspondentes na camada Bronze, conforme o enunciado da atividade.\n",
    "\n",
    "Cada arquivo será processado pela função `ingest_csv()`, resultando em uma tabela Delta dentro de `medalhao.bronze`.\n",
    "\n",
    "Após a execução, podemos verificar usando o script de SQL, no fim do notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07abbbb-56b3-431f-a7a8-024c81429686",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela bronze.ft_consumidores criada com sucesso\n\nTabela bronze.ft_geolocalizacao criada com sucesso\n\nTabela bronze.ft_itens_pedidos criada com sucesso\n\nTabela bronze.ft_pagamentos_pedidos criada com sucesso\n\nTabela bronze.ft_avaliacoes_pedidos criada com sucesso\n\nTabela bronze.ft_pedidos criada com sucesso\n\nTabela bronze.ft_produtos criada com sucesso\n\nTabela bronze.ft_vendedores criada com sucesso\n\nTabela bronze.dm_categoria_produtos_traducao criada com sucesso\n\nIngestão da camada Bronze concluída com sucesso\n"
     ]
    }
   ],
   "source": [
    "arquivos = [\n",
    "    (\"olist_customers_dataset.csv\", \"ft_consumidores\"),\n",
    "    (\"olist_geolocation_dataset.csv\", \"ft_geolocalizacao\"),\n",
    "    (\"olist_order_items_dataset.csv\", \"ft_itens_pedidos\"),\n",
    "    (\"olist_order_payments_dataset.csv\", \"ft_pagamentos_pedidos\"),\n",
    "    (\"olist_order_reviews_dataset.csv\", \"ft_avaliacoes_pedidos\"),\n",
    "    (\"olist_orders_dataset.csv\", \"ft_pedidos\"),\n",
    "    (\"olist_products_dataset.csv\", \"ft_produtos\"),\n",
    "    (\"olist_sellers_dataset.csv\", \"ft_vendedores\"),\n",
    "    (\"product_category_name_translation.csv\", \"dm_categoria_produtos_traducao\")\n",
    "]\n",
    "\n",
    "#loop para ingestão dos 9 arquivos do dataset\n",
    "for nome_arquivo, nome_tabela in arquivos:\n",
    "    ingest_csv(nome_arquivo, nome_tabela)\n",
    "\n",
    "print(\"Ingestão da camada Bronze concluída com sucesso\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4028e4d7-f04e-4f30-a45b-c7e268cb0030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Ingestão da cotação do dólar via API do Banco Central\n",
    "\n",
    "Nesta etapa:\n",
    "- São definidos os parâmetros de data inicio/fim;\n",
    "- O endpoint é montado dinamicamente;\n",
    "- Os dados são requisitados em formato JSON e convertidos em df Spark;\n",
    "- Adiciona-se a coluna `ingestion_timestamp`;\n",
    "- O resultado em `medalhao.bronze.dm_cotacao_dolar`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b3f0d3-3e14-4ecd-949d-6a080ff9efdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela bronze.dm_cotacao_dolar criada com sucesso!\n\n+-------------+--------------------+--------------------+\n|cotacaoCompra|     dataHoraCotacao| ingestion_timestamp|\n+-------------+--------------------+--------------------+\n|       3.2723|2017-01-02 13:07:...|2025-11-11 23:34:...|\n|       3.2626|2017-01-03 13:07:...|2025-11-11 23:34:...|\n|       3.2327|2017-01-04 13:11:...|2025-11-11 23:34:...|\n|       3.2123|2017-01-05 13:04:...|2025-11-11 23:34:...|\n|       3.2051|2017-01-06 13:13:...|2025-11-11 23:34:...|\n+-------------+--------------------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "data_inicio_formatada = \"01-01-2017\"\n",
    "data_fim_formatada = \"12-31-2018\"\n",
    "\n",
    "url = f\"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata/\" \\\n",
    "      f\"CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)\" \\\n",
    "      f\"?@dataInicial='{data_inicio_formatada}'&@dataFinalCotacao='{data_fim_formatada}'\" \\\n",
    "      f\"&$select=dataHoraCotacao,cotacaoCompra&$format=json\"\n",
    "\n",
    "response = requests.get(url) #resposta da API\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"Erro ao acessar a API: {response.status_code} - {response.text}\") # verifica se a requisição foi bem-sucedida\n",
    "\n",
    "data_json = response.json() # converte a resposta em um objeto JSON\n",
    "\n",
    "dados = data_json.get(\"value\", []) # extrai os dados da chave \"value\"\n",
    "if not dados:\n",
    "    raise ValueError(\"Nenhum dado retornado pela API do Banco Central.\") # verifica se há dados\n",
    "\n",
    "df_cotacao = spark.createDataFrame(dados) # cria um DataFrame Spark a partir dos dados\n",
    "\n",
    "df_cotacao = ( # renomeia as colunas e adiciona o timestamp\n",
    "    df_cotacao\n",
    "    .withColumnRenamed(\"dataHoraCotacao\", \"dataHoraCotacao\")\n",
    "    .withColumnRenamed(\"cotacaoCompra\", \"cotacaoCompra\")\n",
    "    .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "df_cotacao.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{catalogo}.{bronze_db_name}.dm_cotacao_dolar\") #formato delta\n",
    "\n",
    "print(\"Tabela bronze.dm_cotacao_dolar criada com sucesso!\\n\")\n",
    "df_cotacao.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd28914-cc31-4019-81e1-80faa6a14679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+\n|       primeira_data|         ultima_data|total_registros|\n+--------------------+--------------------+---------------+\n|2017-01-02 13:07:...|2018-12-31 11:04:...|            499|\n+--------------------+--------------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_cotacao.select( # mostrando a quantidade total de registros \n",
    "    F.min(\"dataHoraCotacao\").alias(\"primeira_data\"),\n",
    "    F.max(\"dataHoraCotacao\").alias(\"ultima_data\"),\n",
    "    F.count(\"*\").alias(\"total_registros\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02bef6a-c80d-4ca5-bc7e-428a9da46ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>bronze</td><td>dm_categoria_produtos_traducao</td><td>false</td></tr><tr><td>bronze</td><td>dm_cotacao_dolar</td><td>false</td></tr><tr><td>bronze</td><td>ft_avaliacoes_pedidos</td><td>false</td></tr><tr><td>bronze</td><td>ft_consumidores</td><td>false</td></tr><tr><td>bronze</td><td>ft_geolocalizacao</td><td>false</td></tr><tr><td>bronze</td><td>ft_itens_pedidos</td><td>false</td></tr><tr><td>bronze</td><td>ft_pagamentos_pedidos</td><td>false</td></tr><tr><td>bronze</td><td>ft_pedidos</td><td>false</td></tr><tr><td>bronze</td><td>ft_produtos</td><td>false</td></tr><tr><td>bronze</td><td>ft_vendedores</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "bronze",
         "dm_categoria_produtos_traducao",
         false
        ],
        [
         "bronze",
         "dm_cotacao_dolar",
         false
        ],
        [
         "bronze",
         "ft_avaliacoes_pedidos",
         false
        ],
        [
         "bronze",
         "ft_consumidores",
         false
        ],
        [
         "bronze",
         "ft_geolocalizacao",
         false
        ],
        [
         "bronze",
         "ft_itens_pedidos",
         false
        ],
        [
         "bronze",
         "ft_pagamentos_pedidos",
         false
        ],
        [
         "bronze",
         "ft_pedidos",
         false
        ],
        [
         "bronze",
         "ft_produtos",
         false
        ],
        [
         "bronze",
         "ft_vendedores",
         false
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "database",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "tableName",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "isTemporary",
            "nullable": false,
            "type": "boolean"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SHOW TABLES IN medalhao.bronze;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47aaf62-538c-42d8-b1c6-e2ff4aa27bb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusão\n",
    "\n",
    "Todas as tabelas foram criadas com sucesso na camada **Bronze**:\n",
    "\n",
    "- ft_consumidores  \n",
    "- ft_geolocalizacao  \n",
    "- ft_itens_pedidos  \n",
    "- ft_pagamentos_pedidos  \n",
    "- ft_avaliacoes_pedidos  \n",
    "- ft_pedidos  \n",
    "- ft_produtos  \n",
    "- ft_vendedores  \n",
    "- dm_categoria_produtos_traducao  \n",
    "- dm_cotacao_dolar  \n",
    "\n",
    "A próxima etapa consiste em transformar, padronizar e enriquecer esses dados na camada **Silver**, que será implementada no notebook `bronze_to_silver`.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4902023497711515,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "land_to_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}